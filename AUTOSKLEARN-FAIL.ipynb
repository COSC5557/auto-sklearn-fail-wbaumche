{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Work around for using Autosklearn in Colab\n",
        "!pip install git+https://github.com/Frankothe196/auto-sklearn.git@python3.10-added-compatibility;"
      ],
      "metadata": {
        "id": "-Wf6MJVj-vrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.model_selection\n",
        "from sklearn.datasets import fetch_openml\n",
        "import sklearn.metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "X, y = fetch_openml(data_id=40691, as_frame=True, return_X_y=True)\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "X = enc.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=42)\n",
        "\n",
        "clf = RandomForestClassifier(random_state=41)\n",
        "clf = clf.fit(X_train, y_train)\n",
        "y_hat = clf.predict(X_test)\n",
        "print(\"RF Accuracy\", sklearn.metrics.accuracy_score(y_test, y_hat))\n",
        "\n",
        "# Redo using pure data, instead of one-hot encoding -- One-Hot encoding is meant\n",
        "# for use on categorical data. However, the values presented as 'observations'\n",
        "# meant to serve as predictors for the 'quality' of the wine are continous real\n",
        "# values. As such, preforming one-hot encoding is not the way to use it.\n",
        "from autosklearn.classification import AutoSklearnClassifier\n",
        "X, y = fetch_openml(data_id=40691, as_frame=True, return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=42)\n",
        "# After checking training and testing data, it was found that while the\n",
        "# training data accuracy was good, the test accuracy was bad. So to avoid\n",
        "# overfitting, trying out a different resampling strategy! As such,\n",
        "# testing cv strategy with normal 10 folds.\n",
        "# In addition; tried running with greater budget, slightly better, but not by a lot.\n",
        "automl = AutoSklearnClassifier(time_left_for_this_task=300,resampling_strategy='cv',resampling_strategy_arguments={\"folds\": 10})\n",
        "automl.fit(X_train, y_train)\n",
        "y_hat = automl.predict(X_test)\n",
        "print(\"AutoML Accuracy On Test\", sklearn.metrics.accuracy_score(y_test, y_hat))\n",
        "\n",
        "print(automl.leaderboard())\n",
        "automl.sprint_statistics()\n",
        "\n",
        "automl.refit(X_train,y_train)\n",
        "y_h = automl.predict(X_train)\n",
        "print(\"AutoML Accuracy on Training\", sklearn.metrics.accuracy_score(y_train, y_h))\n",
        "# This works, with ~0.6725 results as opposed to previous. With extended training\n",
        "# time of 500 iterations, resulting in ~0.675"
      ],
      "metadata": {
        "id": "uMYkZh2X-2XY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}